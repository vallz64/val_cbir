{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Ph3XNBamTs",
        "outputId": "837ce3cf-858b-4494-c0be-aa4d5d742694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install barbar torchsummary\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dataset/cbir_testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX7aZM4Hg7cQ",
        "outputId": "b8348b07-1f90-47d3-93d2-169fa3f8dee4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset/cbir_testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/dataset/cbir_testing\""
      ],
      "metadata": {
        "id": "qNhuD2NzhQNj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d theaayushbajaj/cbir-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QhbYF-Pg-_R",
        "outputId": "55c22788-6c00-4160-f7a8-14ce6c0a1373"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cbir-dataset.zip to /content/drive/MyDrive/dataset/cbir_testing\n",
            " 96% 223M/232M [00:02<00:00, 72.9MB/s]\n",
            "100% 232M/232M [00:02<00:00, 97.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/dataset/cbir_testing/cbir-dataset.zip"
      ],
      "metadata": {
        "id": "r3qdnYgchc5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "from barbar import Bar\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import gc\n",
        "RANDOMSTATE = 0\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "id": "xDKxRyJJcS3s"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, bias=False, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            shortcut = self.downsample(x)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "LkPkyD4pdr2h"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeconvBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, expansion=2, stride=1, upsample=None):\n",
        "        super(DeconvBottleneck, self).__init__()\n",
        "        self.expansion = expansion\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        if stride == 1:\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                                   stride=stride, bias=False, padding=1)\n",
        "        else:\n",
        "            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n",
        "                                            kernel_size=3,\n",
        "                                            stride=stride, bias=False,\n",
        "                                            padding=1,\n",
        "                                            output_padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
        "                               kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.upsample = upsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.upsample is not None:\n",
        "            shortcut = self.upsample(x)\n",
        "\n",
        "        out += shortcut\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "conkv5sPd0Rf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet_autoencoder(nn.Module):\n",
        "    def __init__(self, downblock, upblock, num_layers, n_classes=3):\n",
        "        super(ResNet_autoencoder, self).__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n",
        "        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1],\n",
        "                                           stride=2)\n",
        "        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2],\n",
        "                                           stride=2)\n",
        "        self.layer4 = self._make_downlayer(downblock, 512, num_layers[3],\n",
        "                                           stride=2)\n",
        "\n",
        "        self.uplayer1 = self._make_up_block(\n",
        "            upblock, 512,  num_layers[3], stride=2)\n",
        "        self.uplayer2 = self._make_up_block(\n",
        "            upblock, 256, num_layers[2], stride=2)\n",
        "        self.uplayer3 = self._make_up_block(\n",
        "            upblock, 128, num_layers[1], stride=2)\n",
        "        self.uplayer4 = self._make_up_block(\n",
        "            upblock, 64,  num_layers[0], stride=2)\n",
        "\n",
        "        upsample = nn.Sequential(\n",
        "            nn.ConvTranspose2d(self.in_channels,  # 256\n",
        "                               64,\n",
        "                               kernel_size=1, stride=2,\n",
        "                               bias=False, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "        self.uplayer_top = DeconvBottleneck(\n",
        "            self.in_channels, 64, 1, 2, upsample)\n",
        "\n",
        "        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1,\n",
        "                                          bias=False)\n",
        "\n",
        "    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != init_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, init_channels * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(init_channels * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, init_channels, stride, downsample))\n",
        "        self.in_channels = init_channels * block.expansion\n",
        "        for i in range(1, num_layer):\n",
        "            layers.append(block(self.in_channels, init_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n",
        "        upsample = None\n",
        "        # expansion = block.expansion\n",
        "        if stride != 1 or self.in_channels != init_channels * 2:\n",
        "            upsample = nn.Sequential(\n",
        "                nn.ConvTranspose2d(self.in_channels, init_channels * 2,\n",
        "                                   kernel_size=1, stride=stride,\n",
        "                                   bias=False, output_padding=1),\n",
        "                nn.BatchNorm2d(init_channels * 2),\n",
        "            )\n",
        "        layers = []\n",
        "        for i in range(1, num_layer):\n",
        "            layers.append(block(self.in_channels, init_channels, 4))\n",
        "        layers.append(\n",
        "            block(self.in_channels, init_channels, 2, stride, upsample))\n",
        "        self.in_channels = init_channels * 2\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        return x\n",
        "\n",
        "    def decoder(self, x, image_size):\n",
        "        x = self.uplayer4(x)\n",
        "        x = self.uplayer_top(x)\n",
        "\n",
        "        x = self.conv1_1(x, output_size=image_size)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        img = x\n",
        "        tmp1 = self.encoder(x)\n",
        "        tmp2 = self.decoder(tmp1, img.size())\n",
        "        tmp3 = self.encoder(tmp2)\n",
        "\n",
        "        return tmp1, tmp2, tmp3\n",
        "\n",
        "\n",
        "def ResNet50(**kwargs):\n",
        "    return ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3, **kwargs)\n",
        "\n",
        "\n",
        "def ResNet101(**kwargs):\n",
        "    return ResNet_autoencoder(Bottleneck, [3, 4, 23, 2], 3, **kwargs)"
      ],
      "metadata": {
        "id": "PMV4kqTZd52y"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPu8EE5zeBMq",
        "outputId": "37572ced-cd94-4121-8760-3d9ec3a07f4e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device),(3,512,512))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FB8ls2XeKCD",
        "outputId": "79f78709-6411-426b-e161-33b03c1f212a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 256, 256]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
            "              ReLU-3         [-1, 64, 256, 256]               0\n",
            "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
            "            Conv2d-5         [-1, 64, 128, 128]           4,096\n",
            "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
            "              ReLU-7         [-1, 64, 128, 128]               0\n",
            "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
            "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
            "             ReLU-10         [-1, 64, 128, 128]               0\n",
            "           Conv2d-11        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-12        [-1, 256, 128, 128]             512\n",
            "             ReLU-13        [-1, 256, 128, 128]               0\n",
            "           Conv2d-14        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-15        [-1, 256, 128, 128]             512\n",
            "             ReLU-16        [-1, 256, 128, 128]               0\n",
            "       Bottleneck-17        [-1, 256, 128, 128]               0\n",
            "           Conv2d-18         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-19         [-1, 64, 128, 128]             128\n",
            "             ReLU-20         [-1, 64, 128, 128]               0\n",
            "           Conv2d-21         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-22         [-1, 64, 128, 128]             128\n",
            "             ReLU-23         [-1, 64, 128, 128]               0\n",
            "           Conv2d-24        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-25        [-1, 256, 128, 128]             512\n",
            "             ReLU-26        [-1, 256, 128, 128]               0\n",
            "             ReLU-27        [-1, 256, 128, 128]               0\n",
            "       Bottleneck-28        [-1, 256, 128, 128]               0\n",
            "           Conv2d-29         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-30         [-1, 64, 128, 128]             128\n",
            "             ReLU-31         [-1, 64, 128, 128]               0\n",
            "           Conv2d-32         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-33         [-1, 64, 128, 128]             128\n",
            "             ReLU-34         [-1, 64, 128, 128]               0\n",
            "           Conv2d-35        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-36        [-1, 256, 128, 128]             512\n",
            "             ReLU-37        [-1, 256, 128, 128]               0\n",
            "             ReLU-38        [-1, 256, 128, 128]               0\n",
            "       Bottleneck-39        [-1, 256, 128, 128]               0\n",
            "           Conv2d-40         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-41         [-1, 64, 128, 128]             128\n",
            "             ReLU-42         [-1, 64, 128, 128]               0\n",
            "           Conv2d-43         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-44         [-1, 64, 128, 128]             128\n",
            "             ReLU-45         [-1, 64, 128, 128]               0\n",
            "           Conv2d-46        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-47        [-1, 256, 128, 128]             512\n",
            "             ReLU-48        [-1, 256, 128, 128]               0\n",
            "             ReLU-49        [-1, 256, 128, 128]               0\n",
            " DeconvBottleneck-50        [-1, 256, 128, 128]               0\n",
            "           Conv2d-51         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-52         [-1, 64, 128, 128]             128\n",
            "             ReLU-53         [-1, 64, 128, 128]               0\n",
            "           Conv2d-54         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-55         [-1, 64, 128, 128]             128\n",
            "             ReLU-56         [-1, 64, 128, 128]               0\n",
            "           Conv2d-57        [-1, 256, 128, 128]          16,384\n",
            "      BatchNorm2d-58        [-1, 256, 128, 128]             512\n",
            "             ReLU-59        [-1, 256, 128, 128]               0\n",
            "             ReLU-60        [-1, 256, 128, 128]               0\n",
            " DeconvBottleneck-61        [-1, 256, 128, 128]               0\n",
            "           Conv2d-62         [-1, 64, 128, 128]          16,384\n",
            "      BatchNorm2d-63         [-1, 64, 128, 128]             128\n",
            "             ReLU-64         [-1, 64, 128, 128]               0\n",
            "  ConvTranspose2d-65         [-1, 64, 256, 256]          36,864\n",
            "      BatchNorm2d-66         [-1, 64, 256, 256]             128\n",
            "             ReLU-67         [-1, 64, 256, 256]               0\n",
            "           Conv2d-68        [-1, 128, 256, 256]           8,192\n",
            "      BatchNorm2d-69        [-1, 128, 256, 256]             256\n",
            "             ReLU-70        [-1, 128, 256, 256]               0\n",
            "  ConvTranspose2d-71        [-1, 128, 256, 256]          32,768\n",
            "      BatchNorm2d-72        [-1, 128, 256, 256]             256\n",
            "             ReLU-73        [-1, 128, 256, 256]               0\n",
            " DeconvBottleneck-74        [-1, 128, 256, 256]               0\n",
            "           Conv2d-75         [-1, 64, 256, 256]           8,192\n",
            "      BatchNorm2d-76         [-1, 64, 256, 256]             128\n",
            "             ReLU-77         [-1, 64, 256, 256]               0\n",
            "  ConvTranspose2d-78         [-1, 64, 512, 512]          36,864\n",
            "      BatchNorm2d-79         [-1, 64, 512, 512]             128\n",
            "             ReLU-80         [-1, 64, 512, 512]               0\n",
            "           Conv2d-81         [-1, 64, 512, 512]           4,096\n",
            "      BatchNorm2d-82         [-1, 64, 512, 512]             128\n",
            "             ReLU-83         [-1, 64, 512, 512]               0\n",
            "  ConvTranspose2d-84         [-1, 64, 512, 512]           8,192\n",
            "      BatchNorm2d-85         [-1, 64, 512, 512]             128\n",
            "             ReLU-86         [-1, 64, 512, 512]               0\n",
            " DeconvBottleneck-87         [-1, 64, 512, 512]               0\n",
            "  ConvTranspose2d-88          [-1, 3, 512, 512]             192\n",
            "           Conv2d-89         [-1, 64, 256, 256]           9,408\n",
            "      BatchNorm2d-90         [-1, 64, 256, 256]             128\n",
            "             ReLU-91         [-1, 64, 256, 256]               0\n",
            "        MaxPool2d-92         [-1, 64, 128, 128]               0\n",
            "           Conv2d-93         [-1, 64, 128, 128]           4,096\n",
            "      BatchNorm2d-94         [-1, 64, 128, 128]             128\n",
            "             ReLU-95         [-1, 64, 128, 128]               0\n",
            "           Conv2d-96         [-1, 64, 128, 128]          36,864\n",
            "      BatchNorm2d-97         [-1, 64, 128, 128]             128\n",
            "             ReLU-98         [-1, 64, 128, 128]               0\n",
            "           Conv2d-99        [-1, 256, 128, 128]          16,384\n",
            "     BatchNorm2d-100        [-1, 256, 128, 128]             512\n",
            "            ReLU-101        [-1, 256, 128, 128]               0\n",
            "          Conv2d-102        [-1, 256, 128, 128]          16,384\n",
            "     BatchNorm2d-103        [-1, 256, 128, 128]             512\n",
            "            ReLU-104        [-1, 256, 128, 128]               0\n",
            "      Bottleneck-105        [-1, 256, 128, 128]               0\n",
            "          Conv2d-106         [-1, 64, 128, 128]          16,384\n",
            "     BatchNorm2d-107         [-1, 64, 128, 128]             128\n",
            "            ReLU-108         [-1, 64, 128, 128]               0\n",
            "          Conv2d-109         [-1, 64, 128, 128]          36,864\n",
            "     BatchNorm2d-110         [-1, 64, 128, 128]             128\n",
            "            ReLU-111         [-1, 64, 128, 128]               0\n",
            "          Conv2d-112        [-1, 256, 128, 128]          16,384\n",
            "     BatchNorm2d-113        [-1, 256, 128, 128]             512\n",
            "            ReLU-114        [-1, 256, 128, 128]               0\n",
            "            ReLU-115        [-1, 256, 128, 128]               0\n",
            "      Bottleneck-116        [-1, 256, 128, 128]               0\n",
            "          Conv2d-117         [-1, 64, 128, 128]          16,384\n",
            "     BatchNorm2d-118         [-1, 64, 128, 128]             128\n",
            "            ReLU-119         [-1, 64, 128, 128]               0\n",
            "          Conv2d-120         [-1, 64, 128, 128]          36,864\n",
            "     BatchNorm2d-121         [-1, 64, 128, 128]             128\n",
            "            ReLU-122         [-1, 64, 128, 128]               0\n",
            "          Conv2d-123        [-1, 256, 128, 128]          16,384\n",
            "     BatchNorm2d-124        [-1, 256, 128, 128]             512\n",
            "            ReLU-125        [-1, 256, 128, 128]               0\n",
            "            ReLU-126        [-1, 256, 128, 128]               0\n",
            "      Bottleneck-127        [-1, 256, 128, 128]               0\n",
            "================================================================\n",
            "Total params: 744,512\n",
            "Trainable params: 744,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 3.00\n",
            "Forward/backward pass size (MB): 3950.00\n",
            "Params size (MB): 2.84\n",
            "Estimated Total Size (MB): 3955.84\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing intermediate DataFrame\n",
        "datasetPath = Path('/content/drive/MyDrive/dataset/cbir_testing/dataset')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n",
        "df['image'] = '/content/drive/MyDrive/dataset/cbir_testing/dataset' + df['image'].astype(str)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vbs6YCtyeJ96",
        "outputId": "71961c06-92f3-4826-833e-a9933ea88b04"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               image\n",
              "0  /content/drive/MyDrive/dataset/cbir_testing/da...\n",
              "1  /content/drive/MyDrive/dataset/cbir_testing/da...\n",
              "2  /content/drive/MyDrive/dataset/cbir_testing/da...\n",
              "3  /content/drive/MyDrive/dataset/cbir_testing/da...\n",
              "4  /content/drive/MyDrive/dataset/cbir_testing/da..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6701f0f1-b38e-4cd5-8409-3928f42e3791\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/dataset/cbir_testing/da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/dataset/cbir_testing/da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/dataset/cbir_testing/da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/dataset/cbir_testing/da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/dataset/cbir_testing/da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6701f0f1-b38e-4cd5-8409-3928f42e3791')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6701f0f1-b38e-4cd5-8409-3928f42e3791 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6701f0f1-b38e-4cd5-8409-3928f42e3791');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBIRDataset(Dataset):\n",
        "    def __init__(self, dataFrame):\n",
        "        self.dataFrame = dataFrame\n",
        "        \n",
        "        self.transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, slice):\n",
        "            raise NotImplementedError('slicing is not supported')\n",
        "        \n",
        "        row = self.dataFrame.iloc[key]\n",
        "        image = self.transformations(Image.open(row['image']))\n",
        "        return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataFrame.index)"
      ],
      "metadata": {
        "id": "-dUZ92a1eoPv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    print (\"=> Saving a new best\")\n",
        "    torch.save(state, filename)  # save checkpoint\n",
        "    \n",
        "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
        "    \n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    #valid_loss_min = checkpoint['valid_loss_min']\n",
        "\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "    \n",
        "def train_model(model,  \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                #scheduler, \n",
        "                num_epochs):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    tmp1, tmp2, tmp3 = model(inputs)\n",
        "\n",
        "                    loss1 = criterion(tmp2,inputs.detach())\n",
        "                    loss2 = criterion(tmp3,tmp1.detach())\n",
        "                    loss = loss1 + loss2\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            #if phase == 'train':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, epoch_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                save_checkpoint(state={   \n",
        "                                    'epoch': epoch,\n",
        "                                    'state_dict': model.state_dict(),\n",
        "                                    'best_loss': best_loss,\n",
        "                                    'optimizer_state_dict':optimizer.state_dict()\n",
        "                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, optimizer, epoch_loss"
      ],
      "metadata": {
        "id": "X4xEAXQ1esR4"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intermediate Function to process data from the data retrival class\n",
        "def prepare_data(DF):\n",
        "    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n",
        "    train_set = CBIRDataset(trainDF)\n",
        "    validate_set = CBIRDataset(validateDF)\n",
        "    \n",
        "    return train_set, validate_set"
      ],
      "metadata": {
        "id": "29r0Mg9-e9An"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "NUM_BATCHES = 8\n",
        "RETRAIN = False\n",
        "\n",
        "train_set, validate_set = prepare_data(DF=df)\n",
        "\n",
        "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n",
        "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n",
        "                }\n",
        "\n",
        "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
        "\n",
        "model = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "V7SMXP3jfMsg"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If re-training is required:\n",
        "# Load the old model\n",
        "if RETRAIN == True:\n",
        "    # load the saved checkpoint\n",
        "    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n",
        "    print('Checkpoint Loaded')"
      ],
      "metadata": {
        "id": "qNfBDQBVfPAv"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, loss = train_model(model=model, \n",
        "                    criterion=criterion, \n",
        "                    optimizer=optimizer, \n",
        "                    #scheduler=exp_lr_scheduler,\n",
        "                    num_epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "8pWM5uSPfRvT",
        "outputId": "ed6c9678-70c2-4c97-b006-b4f17e37d684"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/100\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-83aa324bc1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;31m#scheduler=exp_lr_scheduler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     num_epochs=EPOCHS)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-04b24ddf7b1b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/barbar/Bar.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-50-9681f117cfee>\", line 15, in __getitem__\n    image = self.transformations(Image.open(row['image']))\n  File \"/usr/local/lib/python3.7/dist-packages/PIL/Image.py\", line 2843, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/dataset/cbir_testing/dataset4356.jpg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'ResNet_autoencoder_100ep.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "tLpFmZ4kfUJY",
        "outputId": "56964845-743f-4c94-d528-a41f8c16f524"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-bf82ad79411f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             }, 'ResNet_autoencoder_100ep.pt')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4wlVMJG0fW3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cropped_df = df.iloc[:400]\n",
        "# test = CBIRDataset(cropped_df)\n",
        "# testloader = DataLoader(test, batch_size=1, num_workers=1)\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])"
      ],
      "metadata": {
        "id": "juzl7sqxfXL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).to(device)\n",
        "model.load_state_dict(torch.load('../input/resnetcbir-pretrained/ResNet_autoencoder_100ep.pt', map_location=device)['model_state_dict'], strict=False)\n",
        "#model.half()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "h2kpwJoxfa0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor = transformations(Image.open('../input/cbir-dataset/dataset/0.jpg'))\n",
        "# tensor = tensor.to(device)\n",
        "# tensor = tensor.half()"
      ],
      "metadata": {
        "id": "MZ5DjQ-lfi7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#latent_features = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "OCsAEebJfjk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_features(images, transformations):\n",
        "    \n",
        "    latent_features = np.zeros((400,256,128,128))\n",
        "    \n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image))\n",
        "        tensor = tensor.to(device)\n",
        "        #tensor = tensor.half()\n",
        "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
        "\n",
        "#     for i,image in enumerate(tqdm(testloader)):\n",
        "#         tensor = image.to(device)\n",
        "#         latent_features[i] = model.encoder(tensor).detach().numpy()\n",
        "        \n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features"
      ],
      "metadata": {
        "id": "77pMQkQ2flRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = df.image.values[:350]\n",
        "latent_features = get_latent_features(images, transformations)"
      ],
      "metadata": {
        "id": "SSR0bMjcfozE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = list(range(0, 350))\n",
        "feature_dict = dict(zip(indexes,latent_features))\n",
        "index_dict = {'indexes':indexes,'features':latent_features}"
      ],
      "metadata": {
        "id": "MjtQ4ds9fq0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IGiOh9FRftny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(a, b):\n",
        "    # compute and return the euclidean distance between two vectors\n",
        "    return np.linalg.norm(a - b)"
      ],
      "metadata": {
        "id": "niPiC-BFfuLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_search(queryFeatures, index, maxResults=64):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(index[\"features\"])):\n",
        "        # compute the euclidean distance between our query features\n",
        "        # and the features for the current image in our index, then\n",
        "        # update our results list with a 2-tuple consisting of the\n",
        "        # computed distance and the index of the image\n",
        "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
        "        results.append((d, i))\n",
        "    \n",
        "    # sort the results and grab the top ones\n",
        "    results = sorted(results)[:maxResults]\n",
        "    # return the list of results\n",
        "    return results"
      ],
      "metadata": {
        "id": "WD047lv_fxap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_montages(image_list, image_shape, montage_shape):\n",
        "\n",
        "    if len(image_shape) != 2:\n",
        "        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n",
        "    if len(montage_shape) != 2:\n",
        "        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n",
        "    image_montages = []\n",
        "    # start with black canvas to draw images onto\n",
        "    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                          dtype=np.uint8)\n",
        "    cursor_pos = [0, 0]\n",
        "    start_new_img = False\n",
        "    for img in image_list:\n",
        "        if type(img).__module__ != np.__name__:\n",
        "            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n",
        "        start_new_img = False\n",
        "        img = cv2.resize(img, image_shape)\n",
        "        # draw image to black canvas\n",
        "        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n",
        "        cursor_pos[0] += image_shape[0]  # increment cursor x position\n",
        "        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n",
        "            cursor_pos[1] += image_shape[1]  # increment cursor y position\n",
        "            cursor_pos[0] = 0\n",
        "            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n",
        "                cursor_pos = [0, 0]\n",
        "                image_montages.append(montage_image)\n",
        "                # reset black canvas\n",
        "                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                                      dtype=np.uint8)\n",
        "                start_new_img = True\n",
        "    if start_new_img is False:\n",
        "        image_montages.append(montage_image)  # add unfinished montage\n",
        "    return image_montages"
      ],
      "metadata": {
        "id": "itY0yjvAfylR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take the features for the current image, find all similar\n",
        "# images in our dataset, and then initialize our list of result\n",
        "# images\n",
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "queryIdx = 23 # Input Index for which images \n",
        "MAX_RESULTS = 10\n",
        "\n",
        "\n",
        "queryFeatures = latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)\n"
      ],
      "metadata": {
        "id": "PD502iRff5C2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}